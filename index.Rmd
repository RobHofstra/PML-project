---
title: "Human Activity Recognition"
author: "Rob Hofstra"
date: "18 juni 2015"
output: html_document
---

###Reading the data
First of we read in the data set:

```{r reading data,cache=TRUE}
datafile <- "pml-training.csv"
if(!file.exists(datafile)) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, destfile=datafile, method="curl")
  dataDownLoaded <- data()
}
har <- read.csv(datafile, stringsAsFactors = FALSE)
```

###Cleaning the data
We won't show the summary here, because it's over 160 variables and it would consume to much space. Suffice to say that, upon inspecting the summary we notice, there are a lot of variables with NA's. Most of them have 19216 NA's, which is too much, considering there are only 19622 observations. And then there are other's that have DIV/0 values.

Apperently these are all aggregated variables, so we decide, to continue with only the measurements themselves and leave out all aggregaged variables, as well as the date and id variables which are of no use to us.

The cleanup looks as follows:

```{r cleaning data,cache=TRUE}
har$X <- NULL
har$user_name <- NULL
har$raw_timestamp_part_1 <- NULL
har$raw_timestamp_part_2 <- NULL
har$cvtd_timestamp <- NULL
har$new_window <- NULL
har$num_window <- NULL
har$kurtosis_roll_belt <- NULL
har$kurtosis_picth_belt <- NULL
har$kurtosis_yaw_belt <- NULL
har$skewness_roll_belt <- NULL
har$skewness_roll_belt.1 <- NULL
har$skewness_yaw_belt <- NULL
har$max_roll_belt <- NULL
har$max_picth_belt <- NULL
har$max_yaw_belt <- NULL
har$min_roll_belt <- NULL
har$min_pitch_belt <- NULL
har$min_yaw_belt <- NULL
har$amplitude_roll_belt <- NULL
har$amplitude_pitch_belt <- NULL
har$amplitude_yaw_belt <- NULL
har$var_total_accel_belt <- NULL
har$avg_roll_belt <- NULL
har$stddev_roll_belt <- NULL
har$var_roll_belt <- NULL
har$avg_pitch_belt <- NULL
har$stddev_pitch_belt <- NULL
har$var_pitch_belt <- NULL
har$avg_yaw_belt <- NULL
har$stddev_yaw_belt <- NULL
har$var_yaw_belt <- NULL
har$var_accel_arm <- NULL
har$avg_roll_arm <- NULL
har$stddev_roll_arm <- NULL
har$var_roll_arm <- NULL
har$avg_pitch_arm <- NULL
har$stddev_pitch_arm <- NULL
har$var_pitch_arm <- NULL
har$avg_yaw_arm <- NULL
har$stddev_yaw_arm <- NULL
har$var_yaw_arm <- NULL
har$kurtosis_roll_arm <- NULL
har$kurtosis_picth_arm <- NULL
har$kurtosis_yaw_arm <- NULL
har$skewness_roll_arm <- NULL
har$skewness_pitch_arm <- NULL
har$skewness_yaw_arm <- NULL
har$max_roll_arm <- NULL
har$max_picth_arm <- NULL
har$max_yaw_arm <- NULL
har$min_roll_arm <- NULL
har$min_pitch_arm <- NULL
har$min_yaw_arm <- NULL
har$amplitude_roll_arm <- NULL
har$amplitude_pitch_arm <- NULL
har$amplitude_yaw_arm <- NULL
har$kurtosis_roll_dumbbell <- NULL
har$kurtosis_picth_dumbbell <- NULL
har$kurtosis_yaw_dumbbell <- NULL
har$skewness_roll_dumbbell <- NULL
har$skewness_pitch_dumbbell <- NULL
har$skewness_yaw_dumbbell <- NULL
har$max_roll_dumbbell <- NULL
har$max_picth_dumbbell <- NULL
har$max_yaw_dumbbell <- NULL
har$min_roll_dumbbell <- NULL
har$min_pitch_dumbbell <- NULL
har$min_yaw_dumbbell <- NULL
har$amplitude_roll_dumbbell <- NULL
har$amplitude_pitch_dumbbell <- NULL
har$amplitude_yaw_dumbbell <- NULL
har$var_accel_dumbbell <- NULL
har$avg_roll_dumbbell <- NULL
har$stddev_roll_dumbbell <- NULL
har$var_roll_dumbbell <- NULL
har$avg_pitch_dumbbell <- NULL
har$stddev_pitch_dumbbell <- NULL
har$var_pitch_dumbbell <- NULL
har$avg_yaw_dumbbell <- NULL
har$stddev_yaw_dumbbell <- NULL
har$var_yaw_dumbbell <- NULL
har$kurtosis_roll_forearm <- NULL
har$kurtosis_picth_forearm <- NULL
har$kurtosis_yaw_forearm <- NULL
har$skewness_roll_forearm <- NULL
har$skewness_pitch_forearm <- NULL
har$skewness_yaw_forearm <- NULL
har$max_roll_forearm <- NULL
har$max_picth_forearm <- NULL
har$max_yaw_forearm <- NULL
har$min_roll_forearm <- NULL
har$min_pitch_forearm <- NULL
har$min_yaw_forearm <- NULL
har$amplitude_roll_forearm <- NULL
har$amplitude_pitch_forearm <- NULL
har$amplitude_yaw_forearm <- NULL
har$var_accel_forearm <- NULL
har$avg_roll_forearm <- NULL
har$stddev_roll_forearm <- NULL
har$var_roll_forearm <- NULL
har$avg_pitch_forearm <- NULL
har$stddev_pitch_forearm <- NULL
har$var_pitch_forearm <- NULL
har$avg_yaw_forearm <- NULL
har$stddev_yaw_forearm <- NULL
har$var_yaw_forearm <- NULL
har$classe <- as.factor(har$classe)
```

###Splitting the data
Now that we'done the cleaning of the data we'll split the data into a training and testing set:

```{r splitting data, message=FALSE,cache=TRUE}
library(caret)
inTrain <- createDataPartition(y=har$classe, p=0.5, list = FALSE)
training <- har[inTrain,]
testing <- har[-inTrain,]
```

###Fitting the data
With the training data set we can fit several model and try them on the test set, to establish the best model. First we'll try the Tree model:

```{r fitting data rpart,message=FALSE,cache=TRUE}
treeFit <- train(classe ~ ., method = 'rpart', data = training)
```

Let's see what this tree looks like:

```{r viewing tree,message=FALSE}
library(rattle)
fancyRpartPlot(treeFit$finalModel, sub = "")
```

And how it predicts on the testing set:

```{r predicting data rpart,message=FALSE}
library(caret)
treePred <- predict(treeFit,newdata = testing)
confusionMatrix(treePred,testing$classe)
```

We see, it's not performing too well. for instance there's no prediction for Class D and the acuracy is only 0.49.  
So let's see if Random Forest can do better:

```{r fittin data rf,cache=TRUE}
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
rfFit <- train(classe ~ ., method = 'rf', data = training)
stopCluster(cl)
```

And let's see how this random forest predicts:

```{r predictin data rf,cache=TRUE}
rfPred <- predict(rfFit,newdata=testing)
confusionMatrix(rfPred,testing$classe)
```

##Results
Now all classes are predicted and we have an accuracy of 0.9887.
Which means we have an out of sample rate of 1 - 0.9887 = `r 1 - 0.9887`
