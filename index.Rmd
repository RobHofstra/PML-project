---
title: "Human Activity Recognition"
author: "Rob Hofstra"
date: "26 juli 2015"
output: html_document
---

###Reading the data
First we read in the data set:

```{r reading, cache=TRUE}
datafile <- "pml-training.csv"
if(!file.exists(datafile)) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, destfile=datafile, method="curl")
  dataDownLoaded <- data()
}
har <- read.csv(datafile, stringsAsFactors = FALSE)
```

###Cleaning the data
Then we create an overview of the data:

```{r cleaning1}
summary(har)
```

On first inspection, we notice, that the first seven variables aren't relevant to the outcome, so we'll remove them:

```{r cleaning2, cache=TRUE}
har <- har[,-(1:7)]
```

Next we see many variables of class character (e.g. 'kurtosis_roll_belt'). This is no good either, so we remove them as well:

```{r cleaning3, cache=TRUE}
nums <- sapply(har, is.numeric)
nums[length(nums)] <- TRUE     #We want to leave the 'classe' variable in the set
har <- har[,nums]
```

Then we see that many variables with 19216 NA's out of 19622 observations, futhermore it appears that these variables are aggregates. We're gonna remove these as well:

```{r cleaning4, cache=TRUE}
har <- har[,colSums(is.na(har))==0]
```

Finally we'll turn the outcome variable into a factor:

```{r cleaning5, cache=TRUE}
har$classe <- as.factor(har$classe)
```

###Splitting the data
Weâ€™ll split the data into a training and testing set and since we have so many observations we can afford to split it in halve:

```{r splitting, cache=TRUE, message=FALSE}
library(caret)
set.seed(123)
inTrain <- createDataPartition(y=har$classe, p=0.5, list = FALSE)
training <- har[inTrain,]
testing <- har[-inTrain,]
```

###Fitting the data
We'll be using a random forest model, because we know this to be a very powerful method:

```{r fitting1, cache=TRUE, message=FALSE}
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
fit <- train(classe ~ ., method = 'rf', preProcess = 'pca', data = training)
stopCluster(cl)
```

Let's see how well it predicts on the training set:

```{r fitting2, cache=TRUE}
trPred <- predict(fit)
confusionMatrix(trPred,training$classe)
```

This gives a very high accuracy indeed, but we know that it will overfit on the training set, so let's see how well it does on the testing set:

```{r fitting3, cache=TRUE}
tstPred <- predict(fit, newdata = testing)
confusionMatrix(tstPred,testing$classe)
```

###Results
As expected the accuracy on the testing set is a little lower, but with 97% still very good.
Which means we have an out of sample error of 1 - 0,9669 = 0,0331 or just over 3%.